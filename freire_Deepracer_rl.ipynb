{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blessed-preparation",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "sys.path.append(\"common\")\n",
    "sys.path.append(\"./src\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from docker_utils import build_and_push_docker_image\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "from IPython.display import Markdown\n",
    "from markdown_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-august",
   "metadata": {},
   "source": [
    "# Some Initialization Parameters for AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create a new bucket or use an existing bucket with allowed permissions\n",
    "useExistingBucket = True\n",
    "#if true , use exiting models else create a new model, make sure name is unique and add it to our \n",
    "#existing bucket only create a new folder\n",
    "FreireBucket = \"freirebros-deepracer\" #bucketname , this is global bucket (S3 Bucket)\n",
    "\n",
    "#the pretrained folder is incase yo have an old model already trained\n",
    "#if you have non then just call it \"\"\n",
    "#DO NOT PUT A  SLASH AT THE END IT MESSES EVERYTHING UP\n",
    "pretrainedModelPrefix = \"AndreiModels/Model-Andrei-Trial-2-1-DR/andreiModels/Model-Andrei-Trial-2/Fri, 26 Mar 2021 17:20:11 GMT\" #this is the prefix name (a folder within sagemaker S3)\n",
    "#new model directory\n",
    "NewModelPrefix = \"AndreiModels/Model-Andrei-Trial-3-SM\"\n",
    "\n",
    "# Select the instance type\n",
    "instance_type = \"ml.c4.2xlarge\"\n",
    "#instance_type = \"ml.p2.xlarge\"\n",
    "#instance_type = \"ml.c5.4xlarge\"\n",
    "\n",
    "\n",
    "if not FreireBucket: \n",
    "    raise SystemExit(\"Please Enter a Bucket Name\")\n",
    "\n",
    "# Starting SageMaker session\n",
    "sage_session = sagemaker.session.Session(default_bucket = FreireBucket)\n",
    "\n",
    "# Create unique job name.\n",
    "job_name_prefix = 'deepracer-notebook'\n",
    "\n",
    "# Duration of job in seconds (1 hours)\n",
    "job_duration_in_seconds = 3600\n",
    "\n",
    "# AWS Region\n",
    "aws_region = sage_session.boto_region_name\n",
    "if aws_region not in [\"us-west-2\", \"us-east-1\", \"eu-west-1\"]:\n",
    "    raise Exception(\"This notebook uses RoboMaker which is available only in US East (N. Virginia),\"\n",
    "                    \"US West (Oregon) and EU (Ireland). Please switch to one of these regions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-template",
   "metadata": {},
   "source": [
    "# AWS Resources setup \n",
    "## taken from AWS examples - DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch S3 Bucket Resource\n",
    "if not NewModelPrefix:\n",
    "    raise systemExit(\"pleaser Enter a Prefix Name (Folder where the model is stored)\")\n",
    "# S3 bucket\n",
    "if not useExistingBucket:\n",
    "    s3_bucket = sage_session.default_bucket()\n",
    "else:\n",
    "    s3_bucket = FreireBucket\n",
    "# SDK appends the job name and output folder\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "\n",
    "#Ensure that the S3 prefix contains the keyword 'sagemaker'\n",
    "s3_prefix = NewModelPrefix\n",
    "\n",
    "# Get the AWS account id of this account\n",
    "sts = boto3.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()['Account']\n",
    "\n",
    "print(\"Using s3 bucket {}\".format(s3_bucket))\n",
    "print(\"Model checkpoints and other metadata will be stored at: \\ns3://{}/{}\".format(s3_bucket, s3_prefix))\n",
    "\n",
    "#set permisions Resource\n",
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    sagemaker_role = get_execution_role('sagemaker')\n",
    "\n",
    "print(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))\n",
    "\n",
    "#In case reader want to implement themselves\n",
    "display(Markdown(generate_help_for_robomaker_trust_relationship(sagemaker_role)))\n",
    "display(Markdown(generate_s3_write_permission_for_sagemaker_role(sagemaker_role)))\n",
    "display(Markdown(generate_kinesis_create_permission_for_sagemaker_role(sagemaker_role)))\n",
    "\n",
    "#build AWS Docker Resource\n",
    "\n",
    "%time\n",
    "from copy_to_sagemaker_container import get_sagemaker_docker, copy_to_sagemaker_container, get_custom_image_name\n",
    "cpu_or_gpu = 'gpu' if instance_type.startswith('ml.p') else 'cpu'\n",
    "repository_short_name = \"sagemaker-docker-%s\" % cpu_or_gpu\n",
    "custom_image_name = get_custom_image_name(repository_short_name)\n",
    "try:\n",
    "    print(\"Copying files from your notebook to existing sagemaker container\")\n",
    "    sagemaker_docker_id = get_sagemaker_docker(repository_short_name)\n",
    "    copy_to_sagemaker_container(sagemaker_docker_id, repository_short_name)\n",
    "except Exception as e:\n",
    "    print(\"Creating sagemaker container\")\n",
    "    docker_build_args = {\n",
    "        'CPU_OR_GPU': cpu_or_gpu, \n",
    "        'AWS_REGION': boto3.Session().region_name,\n",
    "    }\n",
    "    custom_image_name = build_and_push_docker_image(repository_short_name, build_args=docker_build_args)\n",
    "    print(\"Using ECR image %s\" % custom_image_name)\n",
    "    \n",
    "    \n",
    "\n",
    "#configure VPC Resource\n",
    "ec2 = boto3.client('ec2')\n",
    "\n",
    "#\n",
    "# Check if the user has Deepracer-VPC and use that if its present. This will have all permission.\n",
    "# This VPC will be created when you have used the Deepracer console and created one model atleast\n",
    "# If this is not present. Use the default VPC connnection\n",
    "#\n",
    "deepracer_security_groups = [group[\"GroupId\"] for group in ec2.describe_security_groups()['SecurityGroups']\\\n",
    "                             if group['GroupName'].startswith(\"aws-deepracer-\")]\n",
    "\n",
    "# deepracer_security_groups = False\n",
    "if(deepracer_security_groups):\n",
    "    print(\"Using the DeepRacer VPC stacks. This will be created if you run one training job from console.\")\n",
    "    deepracer_vpc = [vpc['VpcId'] for vpc in ec2.describe_vpcs()['Vpcs'] \\\n",
    "                     if \"Tags\" in vpc for val in vpc['Tags'] \\\n",
    "                     if val['Value'] == 'deepracer-vpc'][0]\n",
    "    deepracer_subnets = [subnet[\"SubnetId\"] for subnet in ec2.describe_subnets()[\"Subnets\"] \\\n",
    "                         if subnet[\"VpcId\"] == deepracer_vpc]\n",
    "else:\n",
    "    print(\"Using the default VPC stacks\")\n",
    "    deepracer_vpc = [vpc['VpcId'] for vpc in ec2.describe_vpcs()['Vpcs'] if vpc[\"IsDefault\"] == True][0]\n",
    "\n",
    "    deepracer_security_groups = [group[\"GroupId\"] for group in ec2.describe_security_groups()['SecurityGroups'] \\\n",
    "                                 if 'VpcId' in group and group[\"GroupName\"] == \"default\" and group[\"VpcId\"] == deepracer_vpc]\n",
    "\n",
    "    deepracer_subnets = [subnet[\"SubnetId\"] for subnet in ec2.describe_subnets()[\"Subnets\"] \\\n",
    "                         if subnet[\"VpcId\"] == deepracer_vpc and subnet['DefaultForAz']==True]\n",
    "\n",
    "print(\"Using VPC:\", deepracer_vpc)\n",
    "print(\"Using security group:\", deepracer_security_groups)\n",
    "print(\"Using subnets:\", deepracer_subnets)\n",
    "\n",
    "#create Route Table resourses\n",
    "#TODO: Explain to customer what CREATE_ROUTE_TABLE is doing\n",
    "CREATE_ROUTE_TABLE = True\n",
    "\n",
    "def create_vpc_endpoint_table():\n",
    "    print(\"Creating \")\n",
    "    try:\n",
    "        route_tables = [route_table[\"RouteTableId\"] for route_table in ec2.describe_route_tables()['RouteTables']\\\n",
    "                        if route_table['VpcId'] == deepracer_vpc]\n",
    "    except Exception as e:\n",
    "        if \"UnauthorizedOperation\" in str(e):\n",
    "            display(Markdown(generate_help_for_s3_endpoint_permissions(sagemaker_role)))\n",
    "        else:\n",
    "            display(Markdown(create_s3_endpoint_manually(aws_region, deepracer_vpc)))\n",
    "        raise e\n",
    "\n",
    "    print(\"Trying to attach S3 endpoints to the following route tables:\", route_tables)\n",
    "    \n",
    "    if not route_tables:\n",
    "        raise Exception((\"No route tables were found. Please follow the VPC S3 endpoint creation \"\n",
    "                         \"guide by clicking the above link.\"))\n",
    "    try:\n",
    "        ec2.create_vpc_endpoint(DryRun=False,\n",
    "                                VpcEndpointType=\"Gateway\",\n",
    "                                VpcId=deepracer_vpc,\n",
    "                                ServiceName=\"com.amazonaws.{}.s3\".format(aws_region),\n",
    "                                RouteTableIds=route_tables)\n",
    "        print(\"S3 endpoint created successfully!\")\n",
    "    except Exception as e:\n",
    "        if \"RouteAlreadyExists\" in str(e):\n",
    "            print(\"S3 endpoint already exists.\")\n",
    "        elif \"UnauthorizedOperation\" in str(e):\n",
    "            display(Markdown(generate_help_for_s3_endpoint_permissions(role)))\n",
    "            raise e\n",
    "        else:\n",
    "            display(Markdown(create_s3_endpoint_manually(aws_region, deepracer_vpc)))\n",
    "            raise e\n",
    "\n",
    "if CREATE_ROUTE_TABLE:\n",
    "    create_vpc_endpoint_table()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-jones",
   "metadata": {},
   "source": [
    "# Set Up Training Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy any custom files such as \n",
    "#custom reward function\n",
    "#meta_data\n",
    "#presets -> action space\n",
    "#attention layer\n",
    "\n",
    "s3_location = \"s3://%s/%s\" % (s3_bucket, s3_prefix)\n",
    "print(s3_location)\n",
    "\n",
    "# Clean up the previously uploaded files\n",
    "!aws s3 rm --recursive {s3_location}\n",
    "\n",
    "!aws s3 cp ./src/artifacts/rewards/andrei_reward_optimized_turns.py {s3_location}/customer_reward_function.py\n",
    "\n",
    "!aws s3 cp ./src/artifacts/actions/front_deep_two_speed_5steering.json {s3_location}/model/model_metadata.json\n",
    "\n",
    "#!aws s3 cp src/markov/presets/default.py {s3_location}/presets/preset.py\n",
    "#!aws s3 cp src/markov/presets/preset_attention_layer.py {s3_location}/presets/preset.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Metric we want to collect\n",
    "metric_definitions = [\n",
    "    # Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=-102.88, Steps=19019, Training iteration=1\n",
    "    {'Name': 'reward-training',\n",
    "     'Regex': '^Training>.*Total reward=(.*?),'},\n",
    "    \n",
    "    # Policy training> Surrogate loss=-0.32664725184440613, KL divergence=7.255815035023261e-06, Entropy=2.83156156539917, training epoch=0, learning_rate=0.00025\n",
    "    {'Name': 'ppo-surrogate-loss',\n",
    "     'Regex': '^Policy training>.*Surrogate loss=(.*?),'},\n",
    "     {'Name': 'ppo-entropy',\n",
    "     'Regex': '^Policy training>.*Entropy=(.*?),'},\n",
    "   \n",
    "    # Testing> Name=main_level/agent, Worker=0, Episode=19, Total reward=1359.12, Steps=20015, Training iteration=2\n",
    "    {'Name': 'reward-testing',\n",
    "     'Regex': '^Testing>.*Total reward=(.*?),'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-jumping",
   "metadata": {},
   "source": [
    "# TRAIN\n",
    "Configure hyper parameters\n",
    "exploration type che k : https://intellabs.github.io/coach/components/exploration_policies/index.html\n",
    "Categorical\n",
    "i will choose EGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RLEstimator(entry_point=\"training_worker.py\",\n",
    "                        source_dir='src',\n",
    "                        image_uri=custom_image_name,\n",
    "                        dependencies=[\"common/\"],\n",
    "                        role=sagemaker_role,\n",
    "                        instance_type=instance_type,\n",
    "                        instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        max_run=job_duration_in_seconds,\n",
    "                        hyperparameters={\n",
    "                            \"s3_bucket\": s3_bucket,\n",
    "                            \"s3_prefix\": s3_prefix,\n",
    "                            \"aws_region\": aws_region,\n",
    "                            \"model_metadata_s3_key\": \"%s/model/model_metadata.json\" % s3_prefix,\n",
    "                            \"reward_function_s3_source\": \"%s/customer_reward_function.py\" % s3_prefix,\n",
    "                            \"batch_size\": \"64\",\n",
    "                            \"num_epochs\": \"10\",\n",
    "                            \"stack_size\": \"1\",\n",
    "                            \"lr\": \"0.0003\",\n",
    "                            \"exploration_type\": \"Greedy\",\n",
    "                            \"e_greedy_value\": \"1\",\n",
    "                            \"epsilon_steps\": \"10000\",\n",
    "                            \"beta_entropy\": \"0.01\",\n",
    "                            \"discount_factor\": \"0.95\",\n",
    "                            \"loss_type\": \"Huber\",\n",
    "                            \"num_episodes_between_training\": \"20\",\n",
    "                            \"max_sample_count\": \"0\",\n",
    "                            \"sampling_frequency\": \"1\"\n",
    "                            ,\"pretrained_s3_bucket\": FreireBucket\n",
    "                            ,\"pretrained_s3_prefix\": pretrainedModelPrefix\n",
    "                        },\n",
    "                        subnets=deepracer_subnets,\n",
    "                        security_group_ids=deepracer_security_groups,\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=False)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Info\n",
    "training_job_arn = estimator.latest_training_job.describe()['TrainingJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-telescope",
   "metadata": {},
   "source": [
    "# AWS Simulation and Robot Resources \n",
    "##  taken from AWS examples - DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kinesis video stream\n",
    "kvs_stream_name = \"dr-kvs-{}\".format(job_name)\n",
    "\n",
    "!aws --region {aws_region} kinesisvideo create-stream --stream-name {kvs_stream_name} --media-type video/h264 --data-retention-in-hours 24\n",
    "print (\"Created kinesis video stream {}\".format(kvs_stream_name))\n",
    "\n",
    "# robomaker\n",
    "robomaker = boto3.client(\"robomaker\")\n",
    "\n",
    "#simulation\n",
    "#we shoould consider usin gazebo version 9\n",
    "robomaker_s3_key = 'robomaker/simulation_ws.tar.gz'\n",
    "robomaker_source = {'s3Bucket': s3_bucket,\n",
    "                    's3Key': robomaker_s3_key,\n",
    "                    'architecture': \"X86_64\"}\n",
    "simulation_software_suite={'name': 'Gazebo',\n",
    "                           'version': '7'}\n",
    "robot_software_suite={'name': 'ROS',\n",
    "                      'version': 'Kinetic'}\n",
    "rendering_engine={'name': 'OGRE',\n",
    "                  'version': '1.x'}\n",
    "\n",
    "if not os.path.exists('./build/output.tar.gz'):\n",
    "    print(\"Using the latest simapp from public s3 bucket\")\n",
    "    # Download Robomaker simApp for the deepracer public s3 bucket\n",
    "    simulation_application_bundle_location = \"s3://deepracer-managed-resources-us-east-1/deepracer-simapp.tar.gz\"\n",
    "    !aws s3 cp {simulation_application_bundle_location} ./\n",
    "\n",
    "    # Remove if the Robomaker sim-app is present in s3 bucket\n",
    "    !aws s3 rm s3://{s3_bucket}/{robomaker_s3_key}\n",
    "\n",
    "    # Uploading the Robomaker SimApp to your S3 bucket\n",
    "    !aws s3 cp ./deepracer-simapp.tar.gz s3://{s3_bucket}/{robomaker_s3_key}\n",
    "\n",
    "    # Cleanup the locally downloaded version of SimApp\n",
    "    !rm deepracer-simapp.tar.gz\n",
    "else:\n",
    "    print(\"Using the simapp from build directory\")\n",
    "    !aws s3 cp ./build/output.tar.gz s3://{s3_bucket}/{robomaker_s3_key}\n",
    "        \n",
    "app_name = \"deepracer-notebook-application\" + strftime(\"%y%m%d-%H%M%S\", gmtime())\n",
    "\n",
    "print(app_name)\n",
    "try:\n",
    "    response = robomaker.create_simulation_application(name=app_name,\n",
    "                                                       sources=[robomaker_source],\n",
    "                                                       simulationSoftwareSuite=simulation_software_suite,\n",
    "                                                       robotSoftwareSuite=robot_software_suite,\n",
    "                                                       renderingEngine=rendering_engine)\n",
    "    simulation_app_arn = response[\"arn\"]\n",
    "    print(\"Created a new simulation app with ARN:\", simulation_app_arn)\n",
    "except Exception as e:\n",
    "    if \"AccessDeniedException\" in str(e):\n",
    "        display(Markdown(generate_help_for_robomaker_all_permissions(role)))\n",
    "        raise e\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "        \n",
    "\n",
    "vpcConfig = {\"subnets\": deepracer_subnets,\n",
    "             \"securityGroups\": deepracer_security_groups,\n",
    "             \"assignPublicIp\": True}\n",
    "\n",
    "responses = []\n",
    "for job_no in range(num_simulation_workers):\n",
    "    client_request_token = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    envriron_vars = {\n",
    "        \"S3_YAML_NAME\": s3_yaml_name,\n",
    "        \"SAGEMAKER_SHARED_S3_PREFIX\": s3_prefix,\n",
    "        \"SAGEMAKER_SHARED_S3_BUCKET\": s3_bucket,\n",
    "        \"WORLD_NAME\": world_name,\n",
    "        \"KINESIS_VIDEO_STREAM_NAME\": kvs_stream_name,\n",
    "        \"APP_REGION\": aws_region,\n",
    "        \"MODEL_METADATA_FILE_S3_KEY\": \"%s/model/model_metadata.json\" % s3_prefix,\n",
    "        \"ROLLOUT_IDX\": str(job_no)\n",
    "    }\n",
    "\n",
    "    simulation_application = {\"application\":simulation_app_arn,\n",
    "                              \"launchConfig\": {\"packageName\": \"deepracer_simulation_environment\",\n",
    "                                               \"launchFile\": \"distributed_training.launch\",\n",
    "                                               \"environmentVariables\": envriron_vars}\n",
    "                             }\n",
    "    response =  robomaker.create_simulation_job(iamRole=sagemaker_role,\n",
    "                                            clientRequestToken=client_request_token,\n",
    "                                            maxJobDurationInSeconds=job_duration_in_seconds,\n",
    "                                            failureBehavior=\"Fail\",\n",
    "                                            simulationApplications=[simulation_application],\n",
    "                                            vpcConfig=vpcConfig\n",
    "                                            )\n",
    "    responses.append(response)\n",
    "    time.sleep(5)\n",
    "    \n",
    "\n",
    "print(\"Created the following jobs:\")\n",
    "job_arns = [response[\"arn\"] for response in responses]\n",
    "for job_arn in job_arns:\n",
    "    print(\"Job ARN\", job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-stage",
   "metadata": {},
   "source": [
    "# Lauch Simulation with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracks you can use, there may be more now need to recheck....\n",
    "# Albert.world                      Mexico_track.world                reinvent_wood.world\n",
    "# AmericasGeneratedInclStart.world  Monaco_building.world             Singapore_building.world\n",
    "# Aragon.world                      Monaco.world                      Singapore_f1.world\n",
    "# Austin.world                      New_York_Track.world              Singapore.world\n",
    "# AWS_track.world                   Oval_track.world                  Spain_track_f1.world\n",
    "# Belille.world                     reInvent2019_track.world          Spain_track.world\n",
    "# Bowtie_track.world                reInvent2019_wide_mirrored.world  Straight_track.world\n",
    "# Canada_Training.world             reInvent2019_wide.world           Tokyo_Training_track.world\n",
    "# China_track.world                 reinvent_base_jeremiah.world      Vegas_track.world\n",
    "# FS_June2020.world                 reinvent_base.world               Virtual_May19_Train_track.world\n",
    "# July_2020.world                   reinvent_carpet.world\n",
    "# LGSWide.world                     reinvent_concrete.world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "s3_yaml_name=\"training_params.yaml\"\n",
    "world_name = \"reInvent2019_track\"\n",
    "# Change this for multiple rollouts. This will invoke the specified number of robomaker jobs to collect experience\n",
    "num_simulation_workers = 1\n",
    "\n",
    "with open(\"./src/artifacts/yaml/training_yaml_template.yaml\", \"r\") as filepointer:\n",
    "    yaml_config = yaml.load(filepointer)\n",
    "\n",
    "yaml_config['WORLD_NAME']                  = world_name\n",
    "yaml_config['SAGEMAKER_SHARED_S3_BUCKET']  = s3_bucket\n",
    "yaml_config['SAGEMAKER_SHARED_S3_PREFIX']  = s3_prefix\n",
    "yaml_config['TRAINING_JOB_ARN']            = training_job_arn\n",
    "yaml_config['METRICS_S3_BUCKET']           = s3_bucket\n",
    "yaml_config['METRICS_S3_OBJECT_KEY']       = \"{}/training_metrics.json\".format(s3_prefix)\n",
    "yaml_config['SIMTRACE_S3_BUCKET']          = s3_bucket\n",
    "yaml_config['SIMTRACE_S3_PREFIX']          = \"{}/iteration-data/training\".format(s3_prefix)\n",
    "yaml_config['AWS_REGION']                  = aws_region\n",
    "yaml_config['ROBOMAKER_SIMULATION_JOB_ACCOUNT_ID'] = account_id\n",
    "yaml_config['KINESIS_VIDEO_STREAM_NAME']   = kvs_stream_name\n",
    "yaml_config['REWARD_FILE_S3_KEY']          = \"{}/customer_reward_function.py\".format(s3_prefix)\n",
    "yaml_config['MODEL_METADATA_FILE_S3_KEY']  = \"{}/model/model_metadata.json\".format(s3_prefix)\n",
    "yaml_config['NUM_WORKERS']                 = num_simulation_workers\n",
    "yaml_config['MP4_S3_BUCKET']               = s3_bucket\n",
    "yaml_config['MP4_S3_OBJECT_PREFIX']        = \"{}/iteration-data/training\".format(s3_prefix)\n",
    "\n",
    "# Race-type supported for training are TIME_TRIAL, OBJECT_AVOIDANCE, HEAD_TO_BOT\n",
    "# If you need to modify more attributes look at the template yaml file\n",
    "race_type = \"TIME_TRIAL\"\n",
    "\n",
    "if race_type == \"OBJECT_AVOIDANCE\":\n",
    "    yaml_config['NUMBER_OF_OBSTACLES']     = \"6\"\n",
    "    yaml_config['RACE_TYPE']               = \"OBJECT_AVOIDANCE\"\n",
    "\n",
    "elif race_type == \"HEAD_TO_BOT\":\n",
    "    yaml_config['NUMBER_OF_BOT_CARS']      = \"6\"\n",
    "    yaml_config['RACE_TYPE']               = \"HEAD_TO_BOT\"\n",
    "\n",
    "# Printing the modified yaml parameter\n",
    "for key, value in yaml_config.items():\n",
    "    print(\"{}: {}\".format(key.ljust(40, ' '), value))\n",
    "\n",
    "# Uploading the modified yaml parameter\n",
    "with open(\"./training_params.yaml\", \"w\") as filepointer:\n",
    "    yaml.dump(yaml_config, filepointer)\n",
    "\n",
    "!aws s3 cp ./training_params.yaml {s3_location}/training_params.yaml\n",
    "!rm training_params.yaml\n",
    "#in case you want to view simulation\n",
    "display(Markdown(generate_robomaker_links(job_arns, aws_region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-logging",
   "metadata": {},
   "source": [
    "# PLOT RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ploting temp directories\n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "training_metrics_file = \"training_metrics.json\"\n",
    "viewModelMetrics = \"deepracer-sagemaker-model1\"\n",
    "training_metrics_path = \"{}/{}\".format(viewModelMetrics, training_metrics_file)\n",
    "wait_for_s3_object(s3_bucket, training_metrics_path, tmp_dir)\n",
    "\n",
    "json_file = \"{}/{}\".format(tmp_dir, training_metrics_file)\n",
    "with open(json_file) as fp:  \n",
    "    data = json.load(fp)\n",
    "\n",
    "df = pd.DataFrame(data['metrics'])\n",
    "x_axis = 'episode'\n",
    "y_axis = 'reward_score'\n",
    "\n",
    "plt = df.plot(x=x_axis,y=y_axis, figsize=(12,5), legend=True, style='b-')\n",
    "plt.set_ylabel(y_axis);\n",
    "plt.set_xlabel(x_axis);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-lincoln",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-therapy",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s3_yaml_name=\"evaluation_params.yaml\"\n",
    "world_name = \"reInvent2019_track\"\n",
    "\n",
    "with open(\"./src/artifacts/yaml/evaluation_yaml_template.yaml\", \"r\") as filepointer:\n",
    "    yaml_config = yaml.load(filepointer)\n",
    "\n",
    "yaml_config['WORLD_NAME']                  = world_name\n",
    "yaml_config['MODEL_S3_BUCKET']             = s3_bucket\n",
    "yaml_config['MODEL_S3_PREFIX']             = s3_prefix\n",
    "yaml_config['AWS_REGION']                  = aws_region\n",
    "yaml_config['METRICS_S3_BUCKET']           = s3_bucket\n",
    "yaml_config['METRICS_S3_OBJECT_KEY']       = \"{}/evaluation_metrics.json\".format(s3_prefix)\n",
    "yaml_config['SIMTRACE_S3_BUCKET']          = s3_bucket\n",
    "yaml_config['SIMTRACE_S3_PREFIX']          = \"{}/iteration-data/evaluation\".format(s3_prefix)\n",
    "yaml_config['ROBOMAKER_SIMULATION_JOB_ACCOUNT_ID'] = account_id\n",
    "yaml_config['NUMBER_OF_TRIALS']            = \"5\"\n",
    "yaml_config['MP4_S3_BUCKET']               = s3_bucket\n",
    "yaml_config['MP4_S3_OBJECT_PREFIX']        = \"{}/iteration-data/evaluation\".format(s3_prefix)\n",
    "\n",
    "# Race-type supported for training are TIME_TRIAL, OBJECT_AVOIDANCE, HEAD_TO_BOT\n",
    "# If you need to modify more attributes look at the template yaml file\n",
    "race_type = \"TIME_TRIAL\"\n",
    "\n",
    "if race_type == \"OBJECT_AVOIDANCE\":\n",
    "    yaml_config['NUMBER_OF_OBSTACLES']     = \"6\"\n",
    "    yaml_config['RACE_TYPE']               = \"OBJECT_AVOIDANCE\"\n",
    "\n",
    "elif race_type == \"HEAD_TO_BOT\":\n",
    "    yaml_config['NUMBER_OF_BOT_CARS']      = \"6\"\n",
    "    yaml_config['RACE_TYPE']               = \"HEAD_TO_BOT\"\n",
    "\n",
    "# Printing the modified yaml parameter\n",
    "for key, value in yaml_config.items():\n",
    "    print(\"{}: {}\".format(key.ljust(40, ' '), value))\n",
    "\n",
    "# Uploading the modified yaml parameter\n",
    "with open(\"./evaluation_params.yaml\", \"w\") as filepointer:\n",
    "    yaml.dump(yaml_config, filepointer)\n",
    "\n",
    "!aws s3 cp ./evaluation_params.yaml {s3_location}/evaluation_params.yaml\n",
    "!rm evaluation_params.yaml\n",
    "\n",
    "num_simulation_workers = 1\n",
    "\n",
    "envriron_vars = {\n",
    "    \"S3_YAML_NAME\": s3_yaml_name,\n",
    "    \"MODEL_S3_PREFIX\": s3_prefix,\n",
    "    \"MODEL_S3_BUCKET\": s3_bucket,\n",
    "    \"WORLD_NAME\": world_name,\n",
    "    \"KINESIS_VIDEO_STREAM_NAME\": kvs_stream_name,\n",
    "    \"APP_REGION\": aws_region,\n",
    "    \"MODEL_METADATA_FILE_S3_KEY\": \"%s/model/model_metadata.json\" % s3_prefix\n",
    "}\n",
    "\n",
    "simulation_application = {\n",
    "    \"application\":simulation_app_arn,\n",
    "    \"launchConfig\": {\n",
    "         \"packageName\": \"deepracer_simulation_environment\",\n",
    "         \"launchFile\": \"evaluation.launch\",\n",
    "         \"environmentVariables\": envriron_vars\n",
    "    }\n",
    "}\n",
    "                            \n",
    "vpcConfig = {\"subnets\": deepracer_subnets,\n",
    "             \"securityGroups\": deepracer_security_groups,\n",
    "             \"assignPublicIp\": True}\n",
    "\n",
    "responses = []\n",
    "for job_no in range(num_simulation_workers):\n",
    "    response =  robomaker.create_simulation_job(clientRequestToken=strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()),\n",
    "                                                outputLocation={ \n",
    "                                                  \"s3Bucket\": s3_bucket,\n",
    "                                                  \"s3Prefix\": s3_prefix\n",
    "                                                },\n",
    "                                                maxJobDurationInSeconds=job_duration_in_seconds,\n",
    "                                                iamRole=sagemaker_role,\n",
    "                                                failureBehavior=\"Fail\",\n",
    "                                                simulationApplications=[simulation_application],\n",
    "                                                vpcConfig=vpcConfig)\n",
    "    responses.append(response)\n",
    "\n",
    "print(\"Created the following jobs:\")\n",
    "job_arns = [response[\"arn\"] for response in responses]\n",
    "for job_arn in job_arns:\n",
    "    print(\"Job ARN\", job_arn)\n",
    "\n",
    "display(Markdown(generate_robomaker_links(job_arns, aws_region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-stuff",
   "metadata": {},
   "source": [
    "## Plot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-jenny",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "evaluation_metrics_file = \"evaluation_metrics.json\"\n",
    "evaluation_metrics_path = \"{}/{}\".format(s3_prefix, evaluation_metrics_file)\n",
    "wait_for_s3_object(s3_bucket, evaluation_metrics_path, tmp_dir)\n",
    "\n",
    "json_file = \"{}/{}\".format(tmp_dir, evaluation_metrics_file)\n",
    "with open(json_file) as fp:  \n",
    "    data = json.load(fp)\n",
    "\n",
    "df = pd.DataFrame(data['metrics'])\n",
    "# Converting milliseconds to seconds\n",
    "df['elapsed_time'] = df['elapsed_time_in_milliseconds']/1000\n",
    "df = df[['trial', 'completion_percentage', 'elapsed_time']]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-listening",
   "metadata": {},
   "source": [
    "# Head To Head Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-stephen",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# S3 bucket\n",
    "s3_bucket_2 = sage_session.default_bucket()\n",
    "\n",
    "# Ensure that the S3 prefix contains the keyword 'sagemaker'\n",
    "# e.g.\"deepracer-notebook-sagemaker-200422-231836\"\n",
    "# Please provide the second agents s3_prefix\n",
    "s3_prefix_2 = \"[FILL OUT PREFIX]\"\n",
    "if not s3_prefix_2:\n",
    "    raise Exception(\"Please provide the second agents s3_prefix and s3_bucket. The prefix would have sagemaker in between\")\n",
    "\n",
    "print(\"Using s3 bucket {}\".format(s3_bucket_2))\n",
    "print(\"Model checkpoints and other metadata will be stored at: \\ns3://{}/{}\".format(s3_bucket_2, s3_prefix_2))\n",
    "\n",
    "\n",
    "s3_yaml_name=\"evaluation_params.yaml\"\n",
    "world_name = \"reInvent2019_track\"\n",
    "\n",
    "with open(\"./src/artifacts/yaml/head2head_yaml_template.yaml\", \"r\") as filepointer:\n",
    "    yaml_config = yaml.load(filepointer)\n",
    "\n",
    "yaml_config['WORLD_NAME']                  = world_name\n",
    "yaml_config['MODEL_S3_BUCKET']             = [s3_bucket,\n",
    "                                              s3_bucket_2]\n",
    "yaml_config['MODEL_S3_PREFIX']             = [s3_prefix,\n",
    "                                              s3_prefix_2]\n",
    "yaml_config['MODEL_METADATA_FILE_S3_KEY']  =[\"{}/model/model_metadata.json\".format(s3_prefix),\n",
    "                                             \"{}/model/model_metadata.json\".format(s3_prefix_2)]\n",
    "yaml_config['AWS_REGION']                  = aws_region\n",
    "yaml_config['METRICS_S3_BUCKET']           = [s3_bucket,\n",
    "                                              s3_bucket_2]\n",
    "yaml_config['METRICS_S3_OBJECT_KEY']       = [\"{}/evaluation_metrics.json\".format(s3_prefix),\n",
    "                                              \"{}/evaluation_metrics.json\".format(s3_prefix_2)]\n",
    "yaml_config['SIMTRACE_S3_BUCKET']          = [s3_bucket,\n",
    "                                              s3_bucket_2]\n",
    "yaml_config['SIMTRACE_S3_PREFIX']          = [\"{}/iteration-data/evaluation\".format(s3_prefix),\n",
    "                                              \"{}/iteration-data/evaluation\".format(s3_prefix_2)]\n",
    "yaml_config['ROBOMAKER_SIMULATION_JOB_ACCOUNT_ID'] = account_id\n",
    "yaml_config['NUMBER_OF_TRIALS']            = \"5\"\n",
    "yaml_config['MP4_S3_BUCKET']               = [s3_bucket,\n",
    "                                              s3_bucket_2]\n",
    "yaml_config['MP4_S3_OBJECT_PREFIX']        = [\"{}/iteration-data/evaluation\".format(s3_prefix),\n",
    "                                              \"{}/iteration-data/evaluation\".format(s3_prefix_2)]\n",
    "\n",
    "# Race-type supported for training are TIME_TRIAL, OBJECT_AVOIDANCE, HEAD_TO_BOT\n",
    "# If you need to modify more attributes look at the template yaml file\n",
    "race_type = \"TIME_TRIAL\"\n",
    "\n",
    "if race_type == \"OBJECT_AVOIDANCE\":\n",
    "    yaml_config['NUMBER_OF_OBSTACLES']     = \"6\"\n",
    "    yaml_config['RACE_TYPE']               = \"OBJECT_AVOIDANCE\"\n",
    "\n",
    "elif race_type == \"HEAD_TO_BOT\":\n",
    "    yaml_config['NUMBER_OF_BOT_CARS']      = \"6\"\n",
    "    yaml_config['RACE_TYPE']               = \"HEAD_TO_BOT\"\n",
    "\n",
    "# Printing the modified yaml parameter\n",
    "for key, value in yaml_config.items():\n",
    "    print(\"{}: {}\".format(key.ljust(40, ' '), value))\n",
    "\n",
    "# Uploading the modified yaml parameter\n",
    "with open(\"./evaluation_params.yaml\", \"w\") as filepointer:\n",
    "    yaml.dump(yaml_config, filepointer)\n",
    "\n",
    "!aws s3 cp ./evaluation_params.yaml {s3_location}/evaluation_params.yaml\n",
    "!rm evaluation_params.yaml\n",
    "\n",
    "num_simulation_workers = 1\n",
    "\n",
    "envriron_vars = {\n",
    "    \"S3_YAML_NAME\": s3_yaml_name,\n",
    "    \"MODEL_S3_PREFIX\": s3_prefix,\n",
    "    \"MODEL_S3_BUCKET\": s3_bucket,\n",
    "    \"WORLD_NAME\": world_name,\n",
    "    \"KINESIS_VIDEO_STREAM_NAME\": kvs_stream_name,\n",
    "    \"APP_REGION\": aws_region,\n",
    "    \"MODEL_METADATA_FILE_S3_KEY\": \"%s/model/model_metadata.json\" % s3_prefix\n",
    "}\n",
    "\n",
    "simulation_application = {\n",
    "    \"application\":simulation_app_arn,\n",
    "    \"launchConfig\": {\n",
    "         \"packageName\": \"deepracer_simulation_environment\",\n",
    "         \"launchFile\": \"evaluation.launch\",\n",
    "         \"environmentVariables\": envriron_vars\n",
    "    }\n",
    "}\n",
    "                            \n",
    "vpcConfig = {\"subnets\": deepracer_subnets,\n",
    "             \"securityGroups\": deepracer_security_groups,\n",
    "             \"assignPublicIp\": True}\n",
    "\n",
    "responses = []\n",
    "for job_no in range(num_simulation_workers):\n",
    "    response =  robomaker.create_simulation_job(clientRequestToken=strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()),\n",
    "                                                outputLocation={ \n",
    "                                                  \"s3Bucket\": s3_bucket,\n",
    "                                                  \"s3Prefix\": s3_prefix\n",
    "                                                },\n",
    "                                                maxJobDurationInSeconds=job_duration_in_seconds,\n",
    "                                                iamRole=sagemaker_role,\n",
    "                                                failureBehavior=\"Fail\",\n",
    "                                                simulationApplications=[simulation_application],\n",
    "                                                vpcConfig=vpcConfig)\n",
    "    responses.append(response)\n",
    "\n",
    "print(\"Created the following jobs:\")\n",
    "job_arns = [response[\"arn\"] for response in responses]\n",
    "for job_arn in job_arns:\n",
    "    print(\"Job ARN\", job_arn)\n",
    "    \n",
    "display(Markdown(generate_robomaker_links(job_arns, aws_region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-jenny",
   "metadata": {},
   "source": [
    "## Plot H2H Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-installation",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "evaluation_metrics_file = \"evaluation_metrics.json\"\n",
    "evaluation_metrics_path = \"{}/{}\".format(s3_prefix, evaluation_metrics_file)\n",
    "wait_for_s3_object(s3_bucket, evaluation_metrics_path, tmp_dir)\n",
    "\n",
    "json_file = \"{}/{}\".format(tmp_dir, evaluation_metrics_file)\n",
    "with open(json_file) as fp:  \n",
    "    data = json.load(fp)\n",
    "\n",
    "df_1 = pd.DataFrame(data['metrics'])\n",
    "# Converting milliseconds to seconds\n",
    "df_1['elapsed_time'] = df_1['elapsed_time_in_milliseconds']/1000\n",
    "df_1 = df_1[['trial', 'completion_percentage', 'elapsed_time']]\n",
    "\n",
    "display(df_1)\n",
    "\n",
    "evaluation_metrics_file = \"evaluation_metrics.json\"\n",
    "evaluation_metrics_path = \"{}/{}\".format(s3_prefix_2, evaluation_metrics_file)\n",
    "wait_for_s3_object(s3_bucket_2, evaluation_metrics_path, tmp_dir)\n",
    "\n",
    "json_file = \"{}/{}\".format(tmp_dir, evaluation_metrics_file)\n",
    "with open(json_file) as fp:  \n",
    "    data = json.load(fp)\n",
    "\n",
    "df_2 = pd.DataFrame(data['metrics'])\n",
    "# Converting milliseconds to seconds\n",
    "df_2['elapsed_time'] = df_2['elapsed_time_in_milliseconds']/1000\n",
    "df_2 = df_2[['trial', 'completion_percentage', 'elapsed_time']]\n",
    "\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-deadline",
   "metadata": {},
   "source": [
    "# CLEAN UP TRAININING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancelling robomaker job\n",
    "for job_arn in job_arns:\n",
    "    robomaker.cancel_simulation_job(job=job_arn)\n",
    "\n",
    "# Stopping sagemaker training job\n",
    "sage_session.sagemaker_client.stop_training_job(TrainingJobName=estimator._current_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-oxford",
   "metadata": {},
   "source": [
    "# Remove Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you ever run this Docker/Cell please do not forget to clean and remove docker\n",
    "#!docker rm -f $(docker ps -a -q);\n",
    "#!docker rmi -f $(docker images -q);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-graphic",
   "metadata": {},
   "source": [
    "# Clean Up simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# robomaker.delete_simulation_application(application=simulation_app_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-sauce",
   "metadata": {},
   "source": [
    "# Clean S3 Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you only want to clean the s3 bucket\n",
    "# sagemaker_s3_folder = \"s3://{}/{}\".format(s3_bucket, s3_prefix)\n",
    "# !aws s3 rm --recursive {sagemaker_s3_folder}\n",
    "\n",
    "# robomaker_s3_folder = \"s3://{}/{}\".format(s3_bucket, job_name)\n",
    "# !aws s3 rm --recursive {robomaker_s3_folder}\n",
    "\n",
    "# robomaker_sim_app = \"s3://{}/{}\".format(s3_bucket, 'robomaker')\n",
    "# !aws s3 rm --recursive {robomaker_sim_app}\n",
    "\n",
    "# model_output = \"s3://{}/{}\".format(s3_bucket, s3_bucket)\n",
    "# !aws s3 rm --recursive {model_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-divide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
